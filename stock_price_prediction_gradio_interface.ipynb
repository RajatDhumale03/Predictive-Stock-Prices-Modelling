{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2fF2jOTG84t",
        "outputId": "a97e2f1b-0d65-4edd-b154-23539e582ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViPvGN2hHnoH",
        "outputId": "a122a271-2de3-467c-9ba1-5bc9197b98c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.5.0-py3-none-any.whl (16.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.7.0 (from gradio)\n",
            "  Downloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydantic>=2.0 (from gradio)\n",
            "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.7.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic>=2.0->gradio)\n",
            "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore (from httpx->gradio)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=21897dc8caa1873f453e1983d219b51b751922f276efcd49163eee487681aa38\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, typing-extensions, tomlkit, shellingham, semantic-version, python-multipart, orjson, h11, colorama, annotated-types, aiofiles, uvicorn, starlette, pydantic-core, httpcore, pydantic, httpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 annotated-types-0.6.0 colorama-0.4.6 fastapi-0.104.1 ffmpy-0.3.1 gradio-4.5.0 gradio-client-0.7.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 orjson-3.9.10 pydantic-2.5.2 pydantic-core-2.14.5 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.27.0 tomlkit-0.12.0 typing-extensions-4.8.0 uvicorn-0.24.0.post1 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If you have a problem with running the python notebook i.e. it throws errors, please restart runtime. Please ensure you are using the GPU before running for a smooth experience"
      ],
      "metadata": {
        "id": "8gAR5KZiKu44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q gradio"
      ],
      "metadata": {
        "id": "_S85CeEDIsaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install typing-extensions --upgrade"
      ],
      "metadata": {
        "id": "5wHCTbJmH-gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "bkL_921HG1Yj",
        "outputId": "6045a904-21bb-4ef5-83cb-677f2ae5b5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://6d3a85ff877314a4e5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6d3a85ff877314a4e5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import random\n",
        "from datetime import datetime as dt\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def predict_stock_price_ARIMA(Stock_name):\n",
        "    Stock_name1 = '/content/drive/MyDrive/ME781/'+ Stock_name.lower()\n",
        "    df = pd.read_csv(f'{Stock_name1}.csv')\n",
        "\n",
        "    # Converting object datatype into datetime datatype\n",
        "    df2 = pd.DataFrame(df['Close'])\n",
        "    df2['Date'] = pd.to_datetime(df['Date']).dt.date\n",
        "\n",
        "    df2.set_index('Date', inplace=True)\n",
        "\n",
        "    first_diffs = df2.Close - df2.Close.shift(1)\n",
        "\n",
        "    # Applying transformation to make data statinoary\n",
        "    df2['FirstDifference'] = first_diffs\n",
        "    df2.dropna(inplace=True)\n",
        "\n",
        "    # Creating the ARIMA model and fitting the values\n",
        "    model = ARIMA(df2.FirstDifference.dropna(), order=(0,1,0))\n",
        "    result = model.fit()\n",
        "\n",
        "    residual = result.resid\n",
        "    residual = residual.cumsum()\n",
        "\n",
        "    # Inversing the transformation to get original data back\n",
        "    a = df2.Close - df2.FirstDifference\n",
        "    diff_results = residual + residual.shift(-1)\n",
        "    diff_results = diff_results + a\n",
        "\n",
        "    df1 = diff_results\n",
        "    df1.dropna(inplace=True)\n",
        "\n",
        "    # Normalizing the data\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    df1 = scaler.fit_transform(np.array(df1).reshape(-1,1))\n",
        "\n",
        "    df1 = df1.reshape(len(df1))\n",
        "\n",
        "    # Defining the train and test data size\n",
        "    training_size=int(len(df1)*0.9)\n",
        "    test_size = len(df1)-training_size\n",
        "\n",
        "    # convert an array of values into a dataset matrix\n",
        "    def create_dataset(dataset, time_step=1):\n",
        "        dataX, dataY = [], []\n",
        "        for i in range(len(dataset)-time_step-1):\n",
        "            a = dataset[i:(i+time_step)]   ###i=0, 0,1,2,3-----99   100\n",
        "            dataX.append(a)\n",
        "            dataY.append(dataset[i + time_step])\n",
        "        return np.array(dataX), np.array(dataY)\n",
        "\n",
        "    time_step = 60\n",
        "    X_train, y_train = create_dataset(df1[:training_size], time_step)\n",
        "    X_test, y_test = create_dataset(df1[training_size:], time_step)\n",
        "\n",
        "    X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
        "\n",
        "    # Creating LSTM model\n",
        "    model=Sequential()\n",
        "    model.add(LSTM(30, input_shape=(X_train.shape[1],1)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error',optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Fiting data into LSTM model\n",
        "    model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=5,batch_size=64,verbose=1)\n",
        "\n",
        "    # Predicting the values from fitted data\n",
        "    train_predict=model.predict(X_train)\n",
        "    test_predict= model.predict(X_test)\n",
        "\n",
        "    # Calculating root mean square error\n",
        "    rmse = math.sqrt(mean_squared_error(y_test,test_predict))\n",
        "\n",
        "    final_test = pd.DataFrame(df2[len(df2) - len(test_predict):])\n",
        "\n",
        "    # Un-normalizing the data\n",
        "    final_test['test'] = scaler.inverse_transform(test_predict)\n",
        "\n",
        "    # Plotting the predictied price wrt original price\n",
        "    plt.figure(figsize=(20,8))\n",
        "    plt.plot(final_test.Close, label='Original Price', color='red')\n",
        "    plt.plot(final_test.test, label='Predicted Price', color='green')\n",
        "    plt.title('Stock of '+Stock_name, fontweight='bold', fontsize=20)\n",
        "    plt.ylabel('Price in Thousand $', fontweight='bold', fontsize=20)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(prop={'size':25})\n",
        "\n",
        "    return rmse, plt\n",
        "\n",
        "def predict_stock_priceLSTM(Stock_name):\n",
        "    # df = pd.read_csv(f'{Stock_name}.csv')\n",
        "    Stock_name1 = '/content/drive/MyDrive/ME781/'+ Stock_name.lower()\n",
        "    df = pd.read_csv(f'{Stock_name1}.csv')\n",
        "    # Dropping the rows with null values\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Creating an array of close values and normalising them\n",
        "    df_close = df.Close.values.reshape(-1,1)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    df_close = scaler.fit_transform(df_close)\n",
        "\n",
        "    # Splitting the data into testing and training sets\n",
        "    train_size = 0.9\n",
        "    training_size=int(len(df_close)*train_size)\n",
        "    test_size=len(df_close)-training_size\n",
        "    train_data,test_data=df_close[0:training_size,:],df_close[training_size:len(df_close),:1]\n",
        "\n",
        "    # Creating batches of dataset of length = timestep\n",
        "    def create_dataset(df, timestep):\n",
        "        df_X, df_y = [], []\n",
        "        for i in range(len(df) - timestep):\n",
        "            df_X.append(df[i:i + timestep, 0])\n",
        "            df_y.append(df[i + timestep, 0])\n",
        "        return np.array(df_X), np.array(df_y)\n",
        "\n",
        "    # Sliding window length= 60 days : Hyperparameter\n",
        "    timestep=60\n",
        "\n",
        "    X_train_lstm, y_train_lstm = create_dataset(train_data, timestep)\n",
        "    X_test_lstm, y_test_lstm = create_dataset(test_data, timestep)\n",
        "\n",
        "    X_train_lstm = X_train_lstm.reshape(X_train_lstm.shape[0],X_train_lstm.shape[1] , 1)\n",
        "    X_test_lstm = X_test_lstm.reshape(X_test_lstm.shape[0],X_test_lstm.shape[1] , 1)\n",
        "\n",
        "    vanilla_lstm = Sequential([\n",
        "        LSTM(units = 128, return_sequences = True, input_shape = (timestep, 1)),\n",
        "        Dropout(rate = 0.15),\n",
        "\n",
        "        LSTM(units = 128, return_sequences = True),\n",
        "        LSTM(units = 128),\n",
        "        Dense(units = 1)\n",
        "    ])\n",
        "    vanilla_lstm.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "    # tensorflow.keras.utils.plot_model(vanilla_lstm)\n",
        "\n",
        "    from tensorflow.keras.callbacks import EarlyStopping\n",
        "    early_stop = EarlyStopping(monitor = 'loss', patience = 1)\n",
        "    vanilla_lstm.fit(X_train_lstm, y_train_lstm, validation_data = (X_test_lstm, y_test_lstm), epochs = 50, batch_size = 64, callbacks = [early_stop])\n",
        "\n",
        "    train_pred = scaler.inverse_transform(vanilla_lstm.predict(X_train_lstm))\n",
        "    y_train_lstm = scaler.inverse_transform(y_train_lstm.reshape(-1,1))\n",
        "\n",
        "    test_pred = scaler.inverse_transform(vanilla_lstm.predict(X_test_lstm))\n",
        "    y_test_lstm = scaler.inverse_transform(y_test_lstm.reshape(-1,1))\n",
        "\n",
        "    mse_test = mean_squared_error(y_test_lstm, test_pred)\n",
        "    rmse_test = np.sqrt(mse_test)\n",
        "\n",
        "    #Restrcuting the output to plot by date\n",
        "    dates = df['Date'][df.shape[0] - y_test_lstm.shape[0]:]\n",
        "    data = zip(dates.tolist(), np.reshape(y_test_lstm, y_test_lstm.shape[0]), np.reshape(test_pred, test_pred.shape[0]))\n",
        "    d = pd.DataFrame(data, columns = ['Dates', 'Actual', 'Predicted'] )\n",
        "    d.set_index('Dates', inplace = True)\n",
        "\n",
        "    plt.figure(figsize = (20,8))\n",
        "    plt.plot(d.Actual, label='Original Price', color = 'red')\n",
        "    plt.plot(d.Predicted, label='Predicted Price', color = 'blue')\n",
        "    plt.title(f'Stock of {Stock_name}', fontweight='bold', fontsize=20)\n",
        "    plt.ylabel('Price in Thousand $', fontweight='bold', fontsize=20)\n",
        "    plt.xticks(np.arange(0, d.Predicted.shape[0], 100), rotation = 45)\n",
        "    plt.legend(prop={'size':25})\n",
        "\n",
        "    return rmse_test, plt\n",
        "\n",
        "def predict_stock_price(Model_name, Stock_name):\n",
        "    if Model_name == 'LSTM':\n",
        "        return predict_stock_priceLSTM(Stock_name)\n",
        "    elif Model_name == 'ARIMA_with_LSTM':\n",
        "        return predict_stock_price_ARIMA(Stock_name)\n",
        "    else:\n",
        "        return \"Invalid model name. Please enter either 'LSTM' or 'ARIMA_with_LSTM'.\"\n",
        "\n",
        "\n",
        "iface = gr.Interface(fn=predict_stock_price, inputs=[\"text\", \"text\"],\n",
        "                     outputs=[\"text\", \"plot\"],\n",
        "                     title=\"Stock Price Predictor: Choose from over 500 stocks to predict\",\n",
        "                     description=\"Enter the name of the stock to predict and the model to use (either 'LSTM' or 'ARIMA_with_LSTM'). The S&P 500 Index comprises 503 stocks, data for which is available online. S&P provides contractions for each company's stocks for convenience: For Example, Google is GOOGL, Amazon is AMZN. Please find a list of these contractions at: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies. Please enter the S&P contracted stock name for predicting the stock of your choice\",\n",
        "                     examples=[[\"LSTM\", \"IBM\"], [\"ARIMA_with_LSTM\", \"GOOGL\"], [\"Output 0\", \"Root Mean Square Test Error\"], [\"Output 1\", \"Predicted Stock Prices over a period of time\"]])\n",
        "iface.launch(debug = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nVn4wue7Hie6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}